{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Word Embeddings\n",
    "#### Hannes Kindbom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "import nltk\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "df_tweets = pd.read_csv(\"Dataset/twitter-airline-sentiment/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568198336651649027</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Delta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GenuineJack</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>JetBlue Ill pass along the advice You guys rock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-18 16:00:14 -0800</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>568438094652956673</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.7036</td>\n",
       "      <td>Lost Luggage</td>\n",
       "      <td>0.7036</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vina_love</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>united I sent you a dm with my file reference ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-19 07:52:57 -0800</td>\n",
       "      <td>ny</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>567858373527470080</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Capt_Smirk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>SouthwestAir Black History Commercial is reall...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 17:29:21 -0800</td>\n",
       "      <td>La Florida</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>569336871853170688</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scoobydoo9749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>SouthwestAir why am I still in Baltimore delta...</td>\n",
       "      <td>[39.1848041, -76.6787131]</td>\n",
       "      <td>2015-02-21 19:24:22 -0800</td>\n",
       "      <td>Tallahassee, FL</td>\n",
       "      <td>America/Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>568839199773732864</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>laurafall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>SouthwestAir SEA to DEN South Sound Volleyball...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-20 10:26:48 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  568198336651649027          positive                        1.0000   \n",
       "1  568438094652956673          negative                        0.7036   \n",
       "2  567858373527470080          positive                        1.0000   \n",
       "3  569336871853170688          negative                        1.0000   \n",
       "4  568839199773732864          positive                        0.6832   \n",
       "\n",
       "  negativereason  negativereason_confidence    airline airline_sentiment_gold  \\\n",
       "0            NaN                        NaN      Delta                    NaN   \n",
       "1   Lost Luggage                     0.7036     United                    NaN   \n",
       "2            NaN                        NaN  Southwest                    NaN   \n",
       "3    Late Flight                     1.0000  Southwest                    NaN   \n",
       "4            NaN                        NaN  Southwest                    NaN   \n",
       "\n",
       "            name negativereason_gold  retweet_count  \\\n",
       "0    GenuineJack                 NaN              0   \n",
       "1      vina_love                 NaN              0   \n",
       "2     Capt_Smirk                 NaN              0   \n",
       "3  scoobydoo9749                 NaN              0   \n",
       "4      laurafall                 NaN              0   \n",
       "\n",
       "                                                text  \\\n",
       "0    JetBlue Ill pass along the advice You guys rock   \n",
       "1  united I sent you a dm with my file reference ...   \n",
       "2  SouthwestAir Black History Commercial is reall...   \n",
       "3  SouthwestAir why am I still in Baltimore delta...   \n",
       "4  SouthwestAir SEA to DEN South Sound Volleyball...   \n",
       "\n",
       "                 tweet_coord              tweet_created   tweet_location  \\\n",
       "0                        NaN  2015-02-18 16:00:14 -0800    Massachusetts   \n",
       "1                        NaN  2015-02-19 07:52:57 -0800               ny   \n",
       "2                        NaN  2015-02-17 17:29:21 -0800       La Florida   \n",
       "3  [39.1848041, -76.6787131]  2015-02-21 19:24:22 -0800  Tallahassee, FL   \n",
       "4                        NaN  2015-02-20 10:26:48 -0800              NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Central Time (US & Canada)  \n",
       "1                       Quito  \n",
       "2  Eastern Time (US & Canada)  \n",
       "3             America/Chicago  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shuffle rows\n",
    "df_tweets = df_tweets.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# Remove Tags\n",
    "#df_tweets[\"text\"] = df_tweets['text'].str.replace('((@|#)\\w+)','') \n",
    "\n",
    "# Remove punctuation \n",
    "df_tweets[\"text\"] = df_tweets['text'].str.replace('[^\\w\\s]','') \n",
    "\n",
    "# Remove numbers\n",
    "df_tweets[\"text\"] = df_tweets['text'].str.replace('[^\\D]','') \n",
    "\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = sum([[nltk.word_tokenize(tok_tweet) for tok_tweet in nltk.sent_tokenize(tweet)] for tweet in df_tweets.text.str.lower()], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW(ngram_range, min_df, x_all, x_train, x_test):\n",
    "    count_vect = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',min_df=min_df, ngram_range=ngram_range)\n",
    "    count_vect.fit(x_all)\n",
    "\n",
    "    xtrain_count =  count_vect.transform(x_train)\n",
    "    xtest_count =  count_vect.transform(x_test)\n",
    "    return xtrain_count, xtest_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = FastText(min_count=3, window=8, workers=12)\n",
    "word_model.build_vocab(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.train(all_tweets, total_examples=word_model.corpus_count, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.save('tweets.wv.fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "#Returns a \"sentence\" vector which is the sum of all word vectors in the sentence. Arg. sentence is a list of words in the sentence\n",
    "def sentence_to_embedding(sentence, a=1000):\n",
    "    embeddings = []\n",
    "    for w in sentence:\n",
    "        try:\n",
    "            # freq is number of occurences in vocab\n",
    "            freq = word_model.wv.vocab[w].count if w in word_model.wv.vocab else 0 \n",
    "            # Get the entity’s representations in vector space, as a 1D numpy array, some normalizing and then append to embeddings\n",
    "            embeddings.append(word_model.wv.get_vector(w)*a/(a+freq))\n",
    "        except:\n",
    "            pass\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros([word_model.wv.vector_size])\n",
    "    return np.sum(embeddings, axis=0)\n",
    "\n",
    "\n",
    "def TransformSentence(sentence):\n",
    "    \n",
    "    tokens = np.asarray([nltk.word_tokenize(tok_sent) for tok_sent in nltk.sent_tokenize(sentence.lower())]).flatten()\n",
    "    output_len = tokens.shape[0]\n",
    "    sent_embeddings = np.zeros([word_model.wv.vector_size])\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "            try:\n",
    "                sent_embeddings = np.add(sent_embeddings, word_model.wv.get_vector(token))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    sent_embeddings = sentence_to_embedding(tokens)\n",
    "    \n",
    "    return sent_embeddings\n",
    "\n",
    "def TransformFeatures(sentences):\n",
    " \n",
    "    sentences_trans = np.array(list(map(TransformSentence, sentences)))\n",
    "    \n",
    "    return sentences_trans\n",
    "\n",
    "def TransformDataFastText(x_train, x_test):\n",
    "    \n",
    "    x_train_trans = TransformFeatures(x_train).reshape((x_train.shape[0], word_model.wv.vector_size))\n",
    "    x_test_trans = TransformFeatures(x_test).reshape((x_test.shape[0], word_model.wv.vector_size))\n",
    "    \n",
    "    return x_train_trans, x_test_trans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_tweets.text.values, df_tweets.airline_sentiment.values, \n",
    "    stratify= df_tweets.airline_sentiment.values, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "word_model = FastText.load('tweets.wv.fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to sentence vectors with fasttext\n",
    "x_train_trans_FT, x_test_trans_FT = TransformDataFastText(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bow, x_test_bow = BoW((1,3), 5, df_tweets[\"text\"], x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save wordembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('Embeddings/x_train_bow', x_train_bow) \n",
    "np.save('Embeddings/x_test_bow', x_test_bow) \n",
    "np.save('Embeddings/x_train_trans_FT', x_train_trans_FT) \n",
    "np.save('Embeddings/x_test_trans_FT', x_test_trans_FT)\n",
    "\n",
    "np.save('Embeddings/y_train', y_train)\n",
    "np.save('Embeddings/y_test', y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo vanilla SVM (Build, train and evaluate models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load saved embeddings\n",
    "x_train_bow = np.load('Embeddings/x_train_bow.npy').item().toarray()\n",
    "x_test_bow = np.load('Embeddings/x_test_bow.npy').item().toarray()\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, clf_name, x_test, y_test):\n",
    "    print(\"Accuracy \" + clf_name + \": \", clf.score(x_test, y_test))\n",
    "    \n",
    "    y_pred = cross_val_predict(clf, x_test, y_test, cv=5) #clf.predict(x_test)\n",
    "    print(\"confusion matrix \"+ clf_name +\": \\n\" , confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "def trainAndEvaluate(clf, x_train, y_train, x_test, y_test, clf_name):\n",
    "    clf.fit(x_train, y_train)\n",
    "    evaluate(clf, clf_name, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy BOW + SVM:  0.7786885245901639\n",
      "confusion matrix BOW + SVM: \n",
      " [[1574  191   71]\n",
      " [ 214  355   51]\n",
      " [ 108   87  277]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.86      0.84      1836\n",
      "     neutral       0.56      0.57      0.57       620\n",
      "    positive       0.69      0.59      0.64       472\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      2928\n",
      "   macro avg       0.70      0.67      0.68      2928\n",
      "weighted avg       0.75      0.75      0.75      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#BoW\n",
    "trainAndEvaluate(clf_svm, x_train_bow, y_train, x_test_bow, y_test, \"BOW + SVM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy FT:  0.7855191256830601\n",
      "confusion matrix FT: \n",
      " [[1617  157   62]\n",
      " [ 234  326   60]\n",
      " [ 126   65  281]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.88      0.85      1836\n",
      "     neutral       0.59      0.53      0.56       620\n",
      "    positive       0.70      0.60      0.64       472\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      2928\n",
      "   macro avg       0.70      0.67      0.68      2928\n",
      "weighted avg       0.75      0.76      0.75      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FastText \n",
    "trainAndEvaluate(clf_svm, x_train_trans_FT, y_train, x_test_trans_FT, y_test, \"FT\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
