{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Word Embeddings\n",
    "#### Hannes Kindbom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile, common_texts\n",
    "import nltk\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "df_tweets = pd.read_csv(\"Dataset/twitter-airline-sentiment/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>What  said</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>plus youve added commercials to the experienc...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>I didnt today Must mean I need to take anothe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>its really aggressive to blast obnoxious ente...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>and its a really big bad thing about it</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                                         What  said         NaN   \n",
       "1   plus youve added commercials to the experienc...         NaN   \n",
       "2   I didnt today Must mean I need to take anothe...         NaN   \n",
       "3   its really aggressive to blast obnoxious ente...         NaN   \n",
       "4            and its a really big bad thing about it         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \\\n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)   \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)   \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)   \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)   \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)   \n",
       "\n",
       "   category  \n",
       "0         1  \n",
       "1         2  \n",
       "2         1  \n",
       "3         0  \n",
       "4         0  "
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shuffle rows\n",
    "#df_tweets = df_tweets.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# Remove Tags\n",
    "df_tweets[\"text\"] = df_tweets['text'].str.replace('((@|#)\\w+)','') \n",
    "\n",
    "# Remove punctuation \n",
    "df_tweets[\"text\"] = df_tweets['text'].str.replace('[^\\w\\s]','') \n",
    "\n",
    "# Remove numbers\n",
    "df_tweets[\"text\"] = df_tweets['text'].str.replace('[^\\D]','') \n",
    "\n",
    "# Add categorical number column\n",
    "df_tweets.airline_sentiment = pd.Categorical(df_tweets.airline_sentiment)\n",
    "df_tweets['category'] = df_tweets.airline_sentiment.cat.codes\n",
    "\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = sum([[nltk.word_tokenize(tok_tweet) for tok_tweet in nltk.sent_tokenize(tweet)] for tweet in df_tweets.text.str.lower()], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW(ngram_range, min_df, x_all, x_train, x_test):\n",
    "    # create a count vectorizer object (BOW-object), min_df removes infrequent words\n",
    "    count_vect = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',min_df=min_df, ngram_range=ngram_range)\n",
    "    count_vect.fit(x_all)\n",
    "    # transform the training and test data using count vectorizer object\n",
    "    xtrain_count =  count_vect.transform(x_train)\n",
    "    xtest_count =  count_vect.transform(x_test)\n",
    "    return xtrain_count, xtest_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Document Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_tweets)]\n",
    "doc_model = Doc2Vec(documents, vector_size=100, epochs=40, window=8, min_count=3, workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "doc_model.save(get_tmpfile(\"tweets_doc2vec_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = FastText(min_count=3, window=8, workers=12)\n",
    "word_model.build_vocab(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.train(all_tweets, total_examples=word_model.corpus_count, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "word_model.save('tweets.wv.fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo vanilla SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "#Returns a \"sentence\" vector which is the sum of all word vectors in the sentence. Arg. sentence is a list of words in the sentence\n",
    "def sentence_to_embedding(sentence, a=1000):\n",
    "    embeddings = []\n",
    "    for w in sentence:\n",
    "        try:\n",
    "            # freq is number of occurences in vocab\n",
    "            freq = word_model.wv.vocab[w].count if w in word_model.wv.vocab else 0 \n",
    "            # Get the entityâ€™s representations in vector space, as a 1D numpy array, some normalizing and then append to embeddings\n",
    "            embeddings.append(word_model.wv.get_vector(w)*a/(a+freq))\n",
    "        except:\n",
    "            pass\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros([word_model.wv.vector_size])\n",
    "    return np.sum(embeddings, axis=0)\n",
    "\n",
    "\n",
    "def TransformSentence(sentence):\n",
    "    \n",
    "    tokens = np.asarray([nltk.word_tokenize(tok_sent) for tok_sent in nltk.sent_tokenize(sentence.lower())]).flatten()\n",
    "    output_len = tokens.shape[0]\n",
    "    sent_embeddings = np.zeros([word_model.wv.vector_size])\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "            try:\n",
    "                sent_embeddings = np.add(sent_embeddings, word_model.wv.get_vector(token))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    sent_embeddings = sentence_to_embedding(tokens)\n",
    "    \n",
    "    return sent_embeddings\n",
    "\n",
    "def TransformFeatures(sentences):\n",
    "    \"\"\"\n",
    "    param: np array of sentences\n",
    "    return: np array (\n",
    "    \"\"\"\n",
    "    sentences_trans = np.array(list(map(TransformSentence, sentences)))\n",
    "    \n",
    "    return sentences_trans\n",
    "\n",
    "def TransformDataFastText(x_train, x_test):\n",
    "    \"\"\"\n",
    "    param: np arrays of text\n",
    "    return: np arrays of numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train_trans = TransformFeatures(x_train).reshape((x_train.shape[0], word_model.wv.vector_size))\n",
    "    x_test_trans = TransformFeatures(x_test).reshape((x_test.shape[0], word_model.wv.vector_size))\n",
    "    \n",
    "    return x_train_trans, x_test_trans\n",
    "\n",
    "def get_doc_vec(sentence):\n",
    "    tokens = [nltk.word_tokenize(tok_sent) for tok_sent in nltk.sent_tokenize(sentence.lower())][0]\n",
    "    doc_vec = doc_model.infer_vector(tokens, steps=40, alpha=0.025)\n",
    "    return doc_vec\n",
    "\n",
    "def TransformDataDoc2Vec(x_test):\n",
    "   \n",
    "    #x_train_trans_D2V = np.array(list(map(get_doc_vec, x_train)))\n",
    "    x_test_trans = np.array(list(map(get_doc_vec, x_test)))\n",
    "    \n",
    "    return x_test_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = FastText.load('tweets.wv.fasttext')\n",
    "doc_model = Doc2Vec.load(get_tmpfile(\"tweets_doc2vec_model\"))\n",
    "doc_model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_tweets.text.values, df_tweets.category.values, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to sentence vectors with fasttext\n",
    "x_train_trans_FT, x_test_trans_FT = TransformDataFastText(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['you', 'make', 'it', 'hard', 'to', 'fly', 'with', 'you', 'delayed', 'over', 'an', 'hour', 'and', 'now', 'the', 'plane', 'is', 'turning', 'around', 'amp', 'heading', 'back', 'to', 'the', 'gate'], [45])\n",
      " You make it hard to fly with you Delayed over an hour and now the plane  is turning around amp heading back to the gate \n"
     ]
    }
   ],
   "source": [
    "print(documents[45])\n",
    "print(df_tweets.text.values[45])\n",
    "\n",
    "# Transform to sentence vectors with doc2vec\n",
    "x_test_D2V = df_tweets.text.values[10000:14000]\n",
    "x_test_trans_D2V = TransformDataDoc2Vec(x_test_D2V)\n",
    "x_train_trans_D2V = np.array([doc_model.docvecs[doc.tags[0]] for doc in documents[0:10000]])\n",
    "y_train_D2V = df_tweets.category.values[0:10000]\n",
    "y_test_D2V = df_tweets.category.values[10000:14000]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 100)\n",
      "(10000, 100)\n",
      "(4000,)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_test_trans_D2V.shape)\n",
    "print(x_train_trans_D2V.shape)\n",
    "print(y_test_D2V.shape)\n",
    "print(y_train_D2V.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67211723\n"
     ]
    }
   ],
   "source": [
    "#Debugging\n",
    "tokens = [nltk.word_tokenize(tok_sent) for tok_sent in nltk.sent_tokenize(df_tweets.text.values[10241].lower())][0]\n",
    "doc_vec = doc_model.infer_vector(tokens, steps=100, alpha=0.025)\n",
    "\n",
    "a = doc_vec\n",
    "b = doc_model.docvecs[10011]\n",
    "dot = np.dot(a, b)\n",
    "norma = np.linalg.norm(a)\n",
    "normb = np.linalg.norm(b)\n",
    "cos = dot / (norma * normb)\n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " It is super frustrating that the folks at the United Ticket Counter in Pittsburg arent honoring their own Media Rate \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'negative'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = 4233\n",
    "print(df_tweets[\"text\"][r])\n",
    "df_tweets[\"airline_sentiment\"][r]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bow, x_test_bow = BoW((1,3), 5, df_tweets[\"text\"], x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BoW SVM\n",
    "clf_bow = svm.SVC()\n",
    "clf_bow.fit(x_train_bow, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FastText SVM\n",
    "clf_FT = svm.SVC()\n",
    "clf_FT.fit(x_train_trans_FT,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 584,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doc2Vec SVM\n",
    "clf_D2V = svm.SVC()\n",
    "clf_D2V.fit(x_train_trans_D2V,y_train_D2V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy BoW SVM:  0.6236338797814208\n"
     ]
    }
   ],
   "source": [
    "#print(\"Accuracy Doc2Vec SVM: \", clf_D2V.score(x_test_trans_D2V,y_test_D2V))\n",
    "#print(\"Accuracy FastText SVM: \", clf_FT.score(x_test_trans_FT,y_test))\n",
    "print(\"Accuracy BoW SVM: \", clf_bow.score(x_test_bow,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix FastText: \n",
      " [[1854    0    0]\n",
      " [ 589    0    0]\n",
      " [ 485    0    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      1.00      0.78      1854\n",
      "           1       0.00      0.00      0.00       589\n",
      "           2       0.00      0.00      0.00       485\n",
      "\n",
      "   micro avg       0.63      0.63      0.63      2928\n",
      "   macro avg       0.21      0.33      0.26      2928\n",
      "weighted avg       0.40      0.63      0.49      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BoW SVM\n",
    "y_pred_bow = clf_bow.predict(x_test_bow)\n",
    "print(\"confusion matrix FastText: \\n\", confusion_matrix(y_test, y_pred_bow))\n",
    "print(classification_report(y_test, y_pred_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix FastText: \n",
      " [[1852    2    0]\n",
      " [ 541   40    8]\n",
      " [ 431    7   47]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      1.00      0.79      1854\n",
      "           1       0.82      0.07      0.13       589\n",
      "           2       0.85      0.10      0.17       485\n",
      "\n",
      "   micro avg       0.66      0.66      0.66      2928\n",
      "   macro avg       0.78      0.39      0.36      2928\n",
      "weighted avg       0.72      0.66      0.56      2928\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# FastText SVM\n",
    "y_pred_FT = clf_FT.predict(x_test_trans_FT)\n",
    "print(\"confusion matrix FastText: \\n\", confusion_matrix(y_test, y_pred_FT))\n",
    "print(classification_report(y_test, y_pred_FT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "confusion matrix Doc2Vec: \n",
      " [[2485    0    0]\n",
      " [ 849    0    0]\n",
      " [ 666    0    0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.62      1.00      0.77      2485\n",
      "           1       0.00      0.00      0.00       849\n",
      "           2       0.00      0.00      0.00       666\n",
      "\n",
      "   micro avg       0.62      0.62      0.62      4000\n",
      "   macro avg       0.21      0.33      0.26      4000\n",
      "weighted avg       0.39      0.62      0.48      4000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# Doc2Vec SVM\n",
    "y_pred_D2V = clf_D2V.predict(x_test_trans_D2V)\n",
    "print(\"confusion matrix Doc2Vec: \\n\", confusion_matrix(y_test_D2V,y_pred_D2V))\n",
    "print(classification_report(y_test_D2V, y_pred_D2V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests - Simon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
