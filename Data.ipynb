{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Word Embeddings\n",
    "#### Hannes Kindbom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.test.utils import get_tmpfile, common_texts\n",
    "import nltk\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "df_tweets = pd.read_csv(\"Dataset/twitter-airline-sentiment/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568198336651649027</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Delta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GenuineJack</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>JetBlue Ill pass along the advice You guys rock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-18 16:00:14 -0800</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>568438094652956673</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.7036</td>\n",
       "      <td>Lost Luggage</td>\n",
       "      <td>0.7036</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vina_love</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>united I sent you a dm with my file reference ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-19 07:52:57 -0800</td>\n",
       "      <td>ny</td>\n",
       "      <td>Quito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>567858373527470080</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Capt_Smirk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>SouthwestAir Black History Commercial is reall...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 17:29:21 -0800</td>\n",
       "      <td>La Florida</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>569336871853170688</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scoobydoo9749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>SouthwestAir why am I still in Baltimore delta...</td>\n",
       "      <td>[39.1848041, -76.6787131]</td>\n",
       "      <td>2015-02-21 19:24:22 -0800</td>\n",
       "      <td>Tallahassee, FL</td>\n",
       "      <td>America/Chicago</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>568839199773732864</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>laurafall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>SouthwestAir SEA to DEN South Sound Volleyball...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-20 10:26:48 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  568198336651649027          positive                        1.0000   \n",
       "1  568438094652956673          negative                        0.7036   \n",
       "2  567858373527470080          positive                        1.0000   \n",
       "3  569336871853170688          negative                        1.0000   \n",
       "4  568839199773732864          positive                        0.6832   \n",
       "\n",
       "  negativereason  negativereason_confidence    airline airline_sentiment_gold  \\\n",
       "0            NaN                        NaN      Delta                    NaN   \n",
       "1   Lost Luggage                     0.7036     United                    NaN   \n",
       "2            NaN                        NaN  Southwest                    NaN   \n",
       "3    Late Flight                     1.0000  Southwest                    NaN   \n",
       "4            NaN                        NaN  Southwest                    NaN   \n",
       "\n",
       "            name negativereason_gold  retweet_count  \\\n",
       "0    GenuineJack                 NaN              0   \n",
       "1      vina_love                 NaN              0   \n",
       "2     Capt_Smirk                 NaN              0   \n",
       "3  scoobydoo9749                 NaN              0   \n",
       "4      laurafall                 NaN              0   \n",
       "\n",
       "                                                text  \\\n",
       "0    JetBlue Ill pass along the advice You guys rock   \n",
       "1  united I sent you a dm with my file reference ...   \n",
       "2  SouthwestAir Black History Commercial is reall...   \n",
       "3  SouthwestAir why am I still in Baltimore delta...   \n",
       "4  SouthwestAir SEA to DEN South Sound Volleyball...   \n",
       "\n",
       "                 tweet_coord              tweet_created   tweet_location  \\\n",
       "0                        NaN  2015-02-18 16:00:14 -0800    Massachusetts   \n",
       "1                        NaN  2015-02-19 07:52:57 -0800               ny   \n",
       "2                        NaN  2015-02-17 17:29:21 -0800       La Florida   \n",
       "3  [39.1848041, -76.6787131]  2015-02-21 19:24:22 -0800  Tallahassee, FL   \n",
       "4                        NaN  2015-02-20 10:26:48 -0800              NaN   \n",
       "\n",
       "                user_timezone  \n",
       "0  Central Time (US & Canada)  \n",
       "1                       Quito  \n",
       "2  Eastern Time (US & Canada)  \n",
       "3             America/Chicago  \n",
       "4  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shuffle rows\n",
    "df_tweets = df_tweets.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# Remove Tags\n",
    "#df_tweets[\"text\"] = df_tweets['text'].str.replace('((@|#)\\w+)','') \n",
    "\n",
    "# Remove punctuation \n",
    "df_tweets[\"text\"] = df_tweets['text'].str.replace('[^\\w\\s]','') \n",
    "\n",
    "# Remove numbers\n",
    "df_tweets[\"text\"] = df_tweets['text'].str.replace('[^\\D]','') \n",
    "\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = sum([[nltk.word_tokenize(tok_tweet) for tok_tweet in nltk.sent_tokenize(tweet)] for tweet in df_tweets.text.str.lower()], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW(ngram_range, min_df, x_all, x_train, x_test):\n",
    "    # create a count vectorizer object (BOW-object), min_df removes infrequent words\n",
    "    count_vect = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',min_df=min_df, ngram_range=ngram_range)\n",
    "    count_vect.fit(x_all)\n",
    "    # transform the training and test data using count vectorizer object\n",
    "    xtrain_count =  count_vect.transform(x_train)\n",
    "    xtest_count =  count_vect.transform(x_test)\n",
    "    return xtrain_count, xtest_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Document Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(all_tweets)]\n",
    "doc_model = Doc2Vec(documents, vector_size=100, epochs=40, window=8, min_count=3, workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model.save(get_tmpfile(\"tweets_doc2vec_model\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = FastText(min_count=3, window=8, workers=12)\n",
    "word_model.build_vocab(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.train(all_tweets, total_examples=word_model.corpus_count, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "word_model.save('tweets.wv.fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo vanilla SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "#Returns a \"sentence\" vector which is the sum of all word vectors in the sentence. Arg. sentence is a list of words in the sentence\n",
    "def sentence_to_embedding(sentence, a=1000):\n",
    "    embeddings = []\n",
    "    for w in sentence:\n",
    "        try:\n",
    "            # freq is number of occurences in vocab\n",
    "            freq = word_model.wv.vocab[w].count if w in word_model.wv.vocab else 0 \n",
    "            # Get the entity’s representations in vector space, as a 1D numpy array, some normalizing and then append to embeddings\n",
    "            embeddings.append(word_model.wv.get_vector(w)*a/(a+freq))\n",
    "        except:\n",
    "            pass\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros([word_model.wv.vector_size])\n",
    "    return np.sum(embeddings, axis=0)\n",
    "\n",
    "\n",
    "def TransformSentence(sentence):\n",
    "    \n",
    "    tokens = np.asarray([nltk.word_tokenize(tok_sent) for tok_sent in nltk.sent_tokenize(sentence.lower())]).flatten()\n",
    "    output_len = tokens.shape[0]\n",
    "    sent_embeddings = np.zeros([word_model.wv.vector_size])\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "            try:\n",
    "                sent_embeddings = np.add(sent_embeddings, word_model.wv.get_vector(token))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    sent_embeddings = sentence_to_embedding(tokens)\n",
    "    \n",
    "    return sent_embeddings\n",
    "\n",
    "def TransformFeatures(sentences):\n",
    "    \"\"\"\n",
    "    param: np array of sentences\n",
    "    return: np array (\n",
    "    \"\"\"\n",
    "    sentences_trans = np.array(list(map(TransformSentence, sentences)))\n",
    "    \n",
    "    return sentences_trans\n",
    "\n",
    "def TransformDataFastText(x_train, x_test):\n",
    "    \"\"\"\n",
    "    param: np arrays of text\n",
    "    return: np arrays of numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train_trans = TransformFeatures(x_train).reshape((x_train.shape[0], word_model.wv.vector_size))\n",
    "    x_test_trans = TransformFeatures(x_test).reshape((x_test.shape[0], word_model.wv.vector_size))\n",
    "    \n",
    "    return x_train_trans, x_test_trans\n",
    "\n",
    "def get_doc_vec(sentence):\n",
    "    tokens = [nltk.word_tokenize(tok_sent) for tok_sent in nltk.sent_tokenize(sentence.lower())][0]\n",
    "    doc_vec = doc_model.infer_vector(tokens, steps=40, alpha=0.025)\n",
    "    return doc_vec\n",
    "\n",
    "def TransformDataDoc2Vec(x_test):\n",
    "   \n",
    "    #x_train_trans_D2V = np.array(list(map(get_doc_vec, x_train)))\n",
    "    x_test_trans = np.array(list(map(get_doc_vec, x_test)))\n",
    "    \n",
    "    return x_test_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = FastText.load('tweets.wv.fasttext')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_model = Doc2Vec.load(get_tmpfile(\"tweets_doc2vec_model\"))\n",
    "doc_model.delete_temporary_training_data(keep_doctags_vectors=True, keep_inference=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_tweets.text.values, df_tweets.airline_sentiment.values, \n",
    "    stratify= df_tweets.airline_sentiment.values, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to sentence vectors with fasttext\n",
    "x_train_trans_FT, x_test_trans_FT = TransformDataFastText(x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TaggedDocument(['you', 'make', 'it', 'hard', 'to', 'fly', 'with', 'you', 'delayed', 'over', 'an', 'hour', 'and', 'now', 'the', 'plane', 'is', 'turning', 'around', 'amp', 'heading', 'back', 'to', 'the', 'gate'], [45])\n",
      " You make it hard to fly with you Delayed over an hour and now the plane  is turning around amp heading back to the gate \n"
     ]
    }
   ],
   "source": [
    "print(documents[45])\n",
    "print(df_tweets.text.values[45])\n",
    "\n",
    "# Transform to sentence vectors with doc2vec\n",
    "x_test_D2V = df_tweets.text.values[10000:14000]\n",
    "x_test_trans_D2V = TransformDataDoc2Vec(x_test_D2V)\n",
    "x_train_trans_D2V = np.array([doc_model.docvecs[doc.tags[0]] for doc in documents[0:10000]])\n",
    "y_train_D2V = df_tweets.airline_sentiment.values[0:10000]\n",
    "y_test_D2V = df_tweets.airline_sentiment.values[10000:14000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 686,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.67211723\n"
     ]
    }
   ],
   "source": [
    "#Debugging\n",
    "tokens = [nltk.word_tokenize(tok_sent) for tok_sent in nltk.sent_tokenize(df_tweets.text.values[10241].lower())][0]\n",
    "doc_vec = doc_model.infer_vector(tokens, steps=100, alpha=0.025)\n",
    "\n",
    "a = doc_vec\n",
    "b = doc_model.docvecs[10011]\n",
    "dot = np.dot(a, b)\n",
    "norma = np.linalg.norm(a)\n",
    "normb = np.linalg.norm(b)\n",
    "cos = dot / (norma * normb)\n",
    "print(cos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(clf, clf_name, x_test, y_test):\n",
    "    print(\"Accuracy \" + clf_name + \": \", clf.score(x_test, y_test))\n",
    "    \n",
    "    y_pred = clf.predict(x_test)\n",
    "    print(\"confusion matrix \"+ clf_name +\": \\n\" , confusion_matrix(y_test, y_pred))\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "def trainAndEvaluate(clf, x_train, y_train, x_test, y_test, clf_name):\n",
    "    clf.fit(x_train, y_train)\n",
    "    evaluate(clf, clf_name, x_test, y_test)\n",
    "    \n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_bow, x_test_bow = BoW((1,3), 5, df_tweets[\"text\"], x_train, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy BOW:  0.7421448087431693\n",
      "confusion matrix BOW: \n",
      " [[1726   85   25]\n",
      " [ 340  234   46]\n",
      " [ 192   67  213]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.94      0.84      1836\n",
      "     neutral       0.61      0.38      0.47       620\n",
      "    positive       0.75      0.45      0.56       472\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      2928\n",
      "   macro avg       0.71      0.59      0.62      2928\n",
      "weighted avg       0.73      0.74      0.72      2928\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#BoW\n",
    "clf_bow = RandomForestClassifier(random_state=0) #svm.SVC()\n",
    "trainAndEvaluate(clf_bow, x_train_bow, y_train, x_test_bow, y_test, \"BOW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy FT:  0.7172131147540983\n",
      "confusion matrix FT: \n",
      " [[1705  105   26]\n",
      " [ 340  230   50]\n",
      " [ 232   75  165]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.75      0.93      0.83      1836\n",
      "     neutral       0.56      0.37      0.45       620\n",
      "    positive       0.68      0.35      0.46       472\n",
      "\n",
      "   micro avg       0.72      0.72      0.72      2928\n",
      "   macro avg       0.66      0.55      0.58      2928\n",
      "weighted avg       0.70      0.72      0.69      2928\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# FastText \n",
    "clf_FT = RandomForestClassifier(random_state=0)#svm.SVC()\n",
    "trainAndEvaluate(clf_FT, x_train_trans_FT, y_train, x_test_trans_FT, y_test, \"FT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy D2V:  0.27875\n",
      "confusion matrix D2V: \n",
      " [[ 326 2125   34]\n",
      " [  55  782   12]\n",
      " [  49  610    7]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.76      0.13      0.22      2485\n",
      "     neutral       0.22      0.92      0.36       849\n",
      "    positive       0.13      0.01      0.02       666\n",
      "\n",
      "   micro avg       0.28      0.28      0.28      4000\n",
      "   macro avg       0.37      0.35      0.20      4000\n",
      "weighted avg       0.54      0.28      0.22      4000\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=None,\n",
       "            oob_score=False, random_state=0, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Doc2Vec \n",
    "clf_D2V = RandomForestClassifier(random_state=0) #svm.SVC()\n",
    "trainAndEvaluate(clf_D2V, x_train_trans_D2V, y_train_D2V, x_test_trans_D2V, y_test_D2V, \"D2V\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests - Simon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
