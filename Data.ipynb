{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation and Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import FastText\n",
    "import nltk\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://www.kaggle.com/crowdflower/twitter-airline-sentiment\n",
    "df_tweets = pd.read_csv(\"Dataset/twitter-airline-sentiment/Tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>568198336651649027</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Delta</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GenuineJack</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Ill pass along the advice You guys rock</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-18 16:00:14 -0800</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>568438094652956673</td>\n",
       "      <td>negative</td>\n",
       "      <td>0.7036</td>\n",
       "      <td>Lost Luggage</td>\n",
       "      <td>0.7036</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>vina_love</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>I sent you a dm with my file reference number...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-19 07:52:57 -0800</td>\n",
       "      <td>ny</td>\n",
       "      <td>Quito</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>567858373527470080</td>\n",
       "      <td>positive</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Capt_Smirk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>Black History Commercial is really sweet Well...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-17 17:29:21 -0800</td>\n",
       "      <td>La Florida</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>569336871853170688</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>scoobydoo9749</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>why am I still in Baltimore  is doing laps ar...</td>\n",
       "      <td>[39.1848041, -76.6787131]</td>\n",
       "      <td>2015-02-21 19:24:22 -0800</td>\n",
       "      <td>Tallahassee, FL</td>\n",
       "      <td>America/Chicago</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>568839199773732864</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6832</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southwest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>laurafall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>SEA to DEN South Sound Volleyball team on its...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-20 10:26:48 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  568198336651649027          positive                        1.0000   \n",
       "1  568438094652956673          negative                        0.7036   \n",
       "2  567858373527470080          positive                        1.0000   \n",
       "3  569336871853170688          negative                        1.0000   \n",
       "4  568839199773732864          positive                        0.6832   \n",
       "\n",
       "  negativereason  negativereason_confidence    airline airline_sentiment_gold  \\\n",
       "0            NaN                        NaN      Delta                    NaN   \n",
       "1   Lost Luggage                     0.7036     United                    NaN   \n",
       "2            NaN                        NaN  Southwest                    NaN   \n",
       "3    Late Flight                     1.0000  Southwest                    NaN   \n",
       "4            NaN                        NaN  Southwest                    NaN   \n",
       "\n",
       "            name negativereason_gold  retweet_count  \\\n",
       "0    GenuineJack                 NaN              0   \n",
       "1      vina_love                 NaN              0   \n",
       "2     Capt_Smirk                 NaN              0   \n",
       "3  scoobydoo9749                 NaN              0   \n",
       "4      laurafall                 NaN              0   \n",
       "\n",
       "                                                text  \\\n",
       "0            Ill pass along the advice You guys rock   \n",
       "1   I sent you a dm with my file reference number...   \n",
       "2   Black History Commercial is really sweet Well...   \n",
       "3   why am I still in Baltimore  is doing laps ar...   \n",
       "4   SEA to DEN South Sound Volleyball team on its...   \n",
       "\n",
       "                 tweet_coord              tweet_created   tweet_location  \\\n",
       "0                        NaN  2015-02-18 16:00:14 -0800    Massachusetts   \n",
       "1                        NaN  2015-02-19 07:52:57 -0800               ny   \n",
       "2                        NaN  2015-02-17 17:29:21 -0800       La Florida   \n",
       "3  [39.1848041, -76.6787131]  2015-02-21 19:24:22 -0800  Tallahassee, FL   \n",
       "4                        NaN  2015-02-20 10:26:48 -0800              NaN   \n",
       "\n",
       "                user_timezone  category  \n",
       "0  Central Time (US & Canada)         2  \n",
       "1                       Quito         0  \n",
       "2  Eastern Time (US & Canada)         2  \n",
       "3             America/Chicago         0  \n",
       "4  Pacific Time (US & Canada)         2  "
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Shuffle rows\n",
    "df_tweets = df_tweets.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "\n",
    "# Remove Tags\n",
    "df_tweets[\"text\"] = df_tweets['text'].str.replace('((@|#)\\w+)','') \n",
    "\n",
    "# Remove punctuation \n",
    "df_tweets[\"text\"] = df_tweets['text'].str.replace('[^\\w\\s]','') \n",
    "\n",
    "# Remove numbers\n",
    "df_tweets[\"text\"] = df_tweets['text'].str.replace('[^\\D]','') \n",
    "\n",
    "# Add categorical number column\n",
    "df_tweets.airline_sentiment = pd.Categorical(df_tweets.airline_sentiment)\n",
    "df_tweets['category'] = df_tweets.airline_sentiment.cat.codes\n",
    "\n",
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = sum([[nltk.word_tokenize(tok_tweet) for tok_tweet in nltk.sent_tokenize(tweet)] for tweet in df_tweets.text.str.lower()], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ill', 'pass', 'along', 'the', 'advice', 'you', 'guys', 'rock']\n",
      "['i', 'sent', 'you', 'a', 'dm', 'with', 'my', 'file', 'reference', 'number', 'i', 'just', 'want', 'to', 'know', 'if', 'someone', 'has', 'located', 'my', 'bag', 'even', 'if', 'its', 'not', 'here', 'yet']\n",
      "['black', 'history', 'commercial', 'is', 'really', 'sweet', 'well', 'done']\n",
      "['why', 'am', 'i', 'still', 'in', 'baltimore', 'is', 'doing', 'laps', 'around', 'us', 'and', 'laughing', 'about', 'it', 'ridiculous']\n",
      "['sea', 'to', 'den', 'south', 'sound', 'volleyball', 'team', 'on', 'its', 'way', 'httptcotncxcldm']\n",
      "['one', 'of', 'your', 'workers', 'refused', 'to', 'give', 'me', 'her', 'name', 'as', 'a', 'reference', 'for', 'my', 'notes', 'her', 'tone', 'amp', 'language', 'was', 'very', 'unprofessional']\n",
      "['seats', 'that', 'were', 'assigned', 'are', 'inappropriate', 'for', 'child', 'this', 'age', 'aa', 'knew', 'age', 'of', 'child']\n",
      "['now', 'you', 'change', 'my', 'gate', 'and', 'dont', 'tell', 'me', 'what', 'the', 'fuck', 'is', 'wrong', 'with', 'you', 'people', 'learn', 'to', 'do', 'your', 'fucken', 'job']\n",
      "['what', 'a', 'mess', 'caused', 'by', 'the', 'computer', 'systems', 'flight', 'in', 'hours', 'late', 'flight', 'and', 'now', 'no', 'gate', 'for', 'us', 'est', 'min', 'wait']\n",
      "['how', 'come', 'you', 'are', 'the', 'only', 'airline', 'of', 'flights', 'last', 'year', 'that', 'makes', 'me', 'check', 'my', 'carryon', 'not', 'even', 'gate', 'checkbaggage', 'claim']\n"
     ]
    }
   ],
   "source": [
    "#Read some tweets\n",
    "for tweet in all_tweets[0:10]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    9178\n",
       "neutral     3099\n",
       "positive    2363\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets.airline_sentiment.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = FastText(min_count=3, window=8, workers=12)\n",
    "word_model.build_vocab(all_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model.train(all_tweets, total_examples=word_model.corpus_count, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "word_model.save('tweets.wv.fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo vanilla SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "#Returns a \"sentence\" vector which is the sum of all word vectors in the sentence. Arg. sentence is a list of words in the sentence\n",
    "def sentence_to_embedding(sentence, a=1000):\n",
    "    embeddings = []\n",
    "    for w in sentence:\n",
    "        try:\n",
    "            # freq is number of occurences in vocab\n",
    "            freq = word_model.wv.vocab[w].count if w in word_model.wv.vocab else 0 \n",
    "            # Get the entity’s representations in vector space, as a 1D numpy array, some normalizing and then append to embeddings\n",
    "            embeddings.append(word_model.wv.get_vector(w)*a/(a+freq))\n",
    "        except:\n",
    "            pass\n",
    "    if len(embeddings) == 0:\n",
    "        return np.zeros([word_model.wv.vector_size])\n",
    "    return np.sum(embeddings, axis=0)\n",
    "\n",
    "\n",
    "def TransformSentence(sentence):\n",
    "    \n",
    "    tokens = np.asarray([nltk.word_tokenize(tok_sent) for tok_sent in nltk.sent_tokenize(sentence.lower())]).flatten()\n",
    "    output_len = tokens.shape[0]\n",
    "    sent_embeddings = np.zeros([word_model.wv.vector_size])\n",
    "    \n",
    "    for i, token in enumerate(tokens):\n",
    "            try:\n",
    "                sent_embeddings = np.add(sent_embeddings, word_model.wv.get_vector(token))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "    sent_embeddings = sentence_to_embedding(tokens)\n",
    "    \n",
    "    return sent_embeddings\n",
    "\n",
    "def TransformFeatures(sentences):\n",
    "    \"\"\"\n",
    "    param: np array of sentences\n",
    "    return: np array (\n",
    "    \"\"\"\n",
    "    sentences_trans = np.array(list(map(TransformSentence, sentences)))\n",
    "    \n",
    "    return sentences_trans\n",
    "\n",
    "def TransformData(x_train, x_test):\n",
    "    \"\"\"\n",
    "    param: np arrays of text\n",
    "    return: np arrays of numbers\n",
    "    \"\"\"\n",
    "    \n",
    "    x_train_trans = TransformFeatures(x_train).reshape((x_train.shape[0], word_model.wv.vector_size))\n",
    "    x_test_trans = TransformFeatures(x_test).reshape((x_test.shape[0], word_model.wv.vector_size))\n",
    "    \n",
    "    \n",
    "    return x_train_trans, x_test_trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_model = FastText.load('tweets.wv.fasttext')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(\n",
    "    df_tweets.text.values, df_tweets.category.values, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2, 0, ..., 0, 0, 1], dtype=int8)"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing stuff\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform to vectors\n",
    "x_train_trans, x_test_trans, = TransformData(x_train, x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hannes/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "  kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC()\n",
    "clf.fit(x_train_trans,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6960382513661202\n"
     ]
    }
   ],
   "source": [
    "print(clf.score(x_test_trans,y_test))\n",
    "#print(f1_score(x_test_trans,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests - Simon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
